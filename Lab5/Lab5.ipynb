{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Unsupervised Learning I - Clustering\n",
    "\n",
    "In this lab, we will cover the following topics:\n",
    "1. Review of algorithms:\n",
    "   * k-Means\n",
    "   * Hierarchical (agglomerative)\n",
    "2. Coding it up:\n",
    "   * k-Means\n",
    "   * Hierarchical (agglomerative)\n",
    "2. Visualization and interpretation of clusters\n",
    "3. Hyperparameter tuning\n",
    "\n",
    "Each section includes basic implementation and questions for further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review: Algorithms\n",
    "\n",
    "### 1.1 k-Means Clustering  \n",
    "**Definition:**  \n",
    "k-Means is a centroid-based clustering algorithm that partitions data into `k` clusters by minimizing intra-cluster variance.\n",
    "\n",
    "**Algorithm Steps:**  \n",
    "1. **Initialize** `k` cluster centroids randomly.  \n",
    "2. **Assign** each data point to the nearest centroid.  \n",
    "3. **Update** centroids by computing the mean of points in each cluster.  \n",
    "4. **Repeat** steps 2-3 until convergence (no further changes in centroids).  \n",
    "\n",
    "**Key Considerations:**  \n",
    "- **Strengths:** Scalable, efficient for large datasets, easy to interpret.  \n",
    "- **Limitations:** Assumes spherical clusters, sensitive to initialization & `k` selection.  \n",
    "- **Best for:** Well-separated, compact, convex clusters.  \n",
    "\n",
    "![k-Means Animation](kmeans.gif)\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Hierarchical (Agglomerative) Clustering  \n",
    "**Definition:**  \n",
    "A bottom-up approach where each point starts as its own cluster, and clusters are iteratively merged based on similarity.\n",
    "\n",
    "**Algorithm Steps:**  \n",
    "1. **Start** with each data point as an individual cluster.  \n",
    "2. **Merge** the closest clusters based on a linkage criterion:  \n",
    "   - **Single Linkage:** Merge clusters with the closest pair of points.  \n",
    "   - **Complete Linkage:** Merge clusters with the farthest pair of points.  \n",
    "   - **Average Linkage:** Merge clusters based on the average distance between all points.  \n",
    "3. **Repeat** until only one cluster remains or a stopping criterion is met.  \n",
    "\n",
    "**Key Considerations:**  \n",
    "- **Strengths:** No need to specify `k`, captures hierarchical relationships.  \n",
    "- **Limitations:** Computationally expensive, sensitive to linkage type.  \n",
    "- **Best for:** Small-to-medium datasets, exploring cluster hierarchy.  \n",
    "\n",
    "![Agglomerative Clustering Animation](hierarch.gif)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Differences  \n",
    "\n",
    "| Feature               | k-Means                     | Hierarchical Clustering (Agglomerative) |\n",
    "|-----------------------|----------------------------|-----------------------------------------|\n",
    "| **Approach**         | Partition-based, centroid-driven | Hierarchical, distance-driven |\n",
    "| **Cluster Shape**    | Prefers spherical clusters | Can detect various cluster shapes |\n",
    "| **Number of Clusters** | Must predefine `k` | Dendrogram can help determine `k` |\n",
    "| **Scalability**      | Efficient for large datasets | Computationally expensive for large `n` |\n",
    "| **Result Type**      | Hard assignments | Hierarchical tree (dendrogram) |\n",
    "\n",
    "---\n",
    "\n",
    "üìå **When to Use Which?**  \n",
    "- Use **k-Means** when you need fast, efficient clustering for large datasets with well-separated groups.  \n",
    "- Use **Hierarchical Clustering** when you want to analyze **cluster relationships** and don't want to predefine `k`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "def plot_clustering_results(X, labels, centers, silhouette_avg, n_clusters=4):\n",
    "    \"\"\"\n",
    "    Plots the clustering results and the silhouette scores for each sample.\n",
    "\n",
    "    Parameters:\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The input data points.\n",
    "    labels : array-like, shape (n_samples,)\n",
    "        The cluster labels for each data point.\n",
    "    centers : array-like, shape (n_clusters, n_features), optional\n",
    "        The coordinates of the cluster centers. If None, no centers are plotted.\n",
    "    silhouette_avg : float\n",
    "        The average silhouette score for all the samples.\n",
    "    n_clusters : int, optional, default=4\n",
    "        The number of clusters.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "    # Plot the clustering result\n",
    "    ax1.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\n",
    "    if centers is not None:\n",
    "        ax1.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')\n",
    "    ax1.set_xlabel('Feature 1')\n",
    "    ax1.set_ylabel('Feature 2')\n",
    "    ax1.set_title('Clustering Result')\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = plt.cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax2.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ax2.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        y_lower = y_upper + 10\n",
    "\n",
    "    ax2.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax2.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax2.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    ax2.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_xticks(np.arange(-0.1, 1.1, 0.2))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering Algorithms\n",
    "\n",
    "### 2.1 k-Means Clustering\n",
    "\n",
    "We will start by implementing k-Means clustering. We will also explore different parameters and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=4, random_state=42)\n",
    "\n",
    "# Train a k-Means clustering model\n",
    "kmeans = KMeans(n_clusters=4, max_iter=5, random_state=42)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "silhouette_avg = silhouette_score(X, y_kmeans)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Plot the clustering results using the custom function\n",
    "centers = kmeans.cluster_centers_\n",
    "plot_clustering_results(X, y_kmeans, centers, silhouette_avg, n_clusters=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans?\n",
    "silhouette_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for Exploration\n",
    "\n",
    "1. How does changing the number of clusters (`n_clusters`) affect the clustering results?\n",
    "2. What happens to the silhouette score when you increase `max_iter`?\n",
    "3. How does the clustering performance change if you use a different random seed for data generation or change the initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Hierarchical Clustering\n",
    "\n",
    "Next, we will implement hierarchical clustering. We will also explore different parameters and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=4, random_state=42)\n",
    "\n",
    "# Perform agglomerative clustering\n",
    "Z = linkage(X, 'ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Agglomerative)')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Train an agglomerative clustering model\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=4)\n",
    "y_agg = agg_clustering.fit_predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "silhouette_avg = silhouette_score(X, y_agg)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Plot the clustering results using the custom function\n",
    "plot_clustering_results(X, y_agg, None, silhouette_avg, n_clusters=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AgglomerativeClustering?\n",
    "linkage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for Exploration\n",
    "\n",
    "1. How does changing the linkage criterion (e.g., `single`, `complete`, `average`, `centroid`) affect the clustering results?\n",
    "2. What happens to the silhouette score when you change the number of clusters?\n",
    "3. How does the clustering performance change if you use a different random seed for data generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualization and Interpretation of Clusters\n",
    "\n",
    "We will visualize and interpret the clusters formed by different clustering algorithms. We will also explore different visualization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=3, random_state=42)\n",
    "\n",
    "# Train a k-Means clustering model\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "# Perform agglomerative clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=4)\n",
    "y_agg = agg_clustering.fit_predict(X)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Plot the k-Means clustering result\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "ax1.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_title('k-Means Clustering with Cluster Centers')\n",
    "\n",
    "# Plot the agglomerative clustering result\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=y_agg, s=50, cmap='viridis')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "ax2.set_title('Agglomerative Clustering')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print the silhouette scores\n",
    "silhouette_avg_kmeans = silhouette_score(X, y_kmeans)\n",
    "silhouette_avg_agg = silhouette_score(X, y_agg)\n",
    "print(f\"Silhouette Score (k-Means): {silhouette_avg_kmeans}\")\n",
    "print(f\"Silhouette Score (Agglomerative): {silhouette_avg_agg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for Exploration\n",
    "\n",
    "1. How do the clusters formed by k-Means and agglomerative clustering compare?\n",
    "2. Another visualization for clustering discussed in class is the silhouette plot - how does this plot change when you make changes to `cluster_std` in `make_blobs`? \n",
    "3. How does the choice of features affect the clustering results and their interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning\n",
    "\n",
    "We will perform hyperparameter tuning to find the best parameters for each clustering algorithm. We will also explore different evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grids\n",
    "param_grid_kmeans = {'n_clusters': [2, 3, 4, 5]}\n",
    "param_grid_agg = {'n_clusters': [2, 3, 4, 5], 'linkage': ['ward', 'complete', 'average']}\n",
    "\n",
    "# Perform grid search for agglomerative clustering\n",
    "def custom_silhouette_scorer(estimator, X):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    return silhouette_score(X, labels)\n",
    "\n",
    "# Perform grid search for k-Means\n",
    "grid_search_kmeans = GridSearchCV(KMeans(random_state=42), param_grid_kmeans, cv=5, scoring=custom_silhouette_scorer)\n",
    "grid_search_kmeans.fit(X)\n",
    "\n",
    "grid_search_agg = GridSearchCV(AgglomerativeClustering(), param_grid_agg, cv=5, scoring=custom_silhouette_scorer)\n",
    "grid_search_agg.fit(X)\n",
    "\n",
    "# Print best parameters and scores\n",
    "print(f\"Best parameters for k-Means: {grid_search_kmeans.best_params_}\")\n",
    "print(f\"Best silhouette score for k-Means: {grid_search_kmeans.best_score_}\")\n",
    "\n",
    "print(f\"Best parameters for Agglomerative Clustering: {grid_search_agg.best_params_}\")\n",
    "print(f\"Best silhouette score for Agglomerative Clustering: {grid_search_agg.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for Exploration\n",
    "\n",
    "1. How does hyperparameter tuning improve the performance of each clustering algorithm?\n",
    "2. What are the best hyperparameters found for each clustering algorithm, and how do they compare to the default parameters?\n",
    "3. How does the choice of evaluation metric (e.g., silhouette score) impact the results of hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "1. Try implementing an elbow plot or gap statistic plot. Does the plot agree with best parameters obtained using `GridSearchCV`?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Recall:\n",
    "### üîπ Elbow Method  \n",
    "The **Elbow Plot** helps determine the optimal number of clusters (**k**) by plotting:  \n",
    "- **X-axis:** Number of clusters (\\(k\\))  \n",
    "- **Y-axis:** Within-Cluster Sum of Squares (WCSS)  \n",
    "\n",
    "#### üîç **Interpretation:**  \n",
    "- **Look for the \"elbow\"**‚Äîthe point where the WCSS **stops decreasing sharply**.  \n",
    "- Before this point, adding clusters significantly reduces WCSS.  \n",
    "- After this point, diminishing returns indicate that additional clusters provide **little extra benefit**. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Gap Statistic  \n",
    "The **Gap Statistic Plot** compares the clustering structure in real data to a **randomized reference** distribution. It plots:  \n",
    "- **X-axis:** Number of clusters (\\(k\\))  \n",
    "- **Y-axis:** Gap statistic (higher means better clustering structure)  \n",
    "\n",
    "#### üîç **Interpretation:**  \n",
    "- The **optimal \\(k\\)** is where the **gap statistic is maximized**.  \n",
    "- If the gap statistic **plateaus** or **starts decreasing**, adding clusters is unnecessary. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
